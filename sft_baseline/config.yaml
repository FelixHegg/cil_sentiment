model: "Qwen/Qwen2.5-3B-Instruct"
path_data: "data"
gradient_checkpointing: true
use_reentrant: false
gradient_accumulation_steps: 8
per_device_train_batch_size: 8
max_seq_length: 4096
num_train_epochs: 1
learning_rate: 0.00005
optim: "adamw_torch"
save_steps: 10
load_in_4bit: true
lora_rank: 16
target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
lora_alpha: 32
lora_dropout: 0.1
exp_dir: "outputs"
